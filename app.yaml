resnet56: &resnet56
  model_package: examples.vision.resnet_model
  model_class: ResNet
  dataio_package: examples.vision.cifar10
  dataio_class: CIFAR10

  batch_size: 256
  
  n_size: 9
  num_classes: 10

resnet110: &resnet110
  <<: *resnet56
  n_size: 18

bert: &bert
  model_package: rosetta.models.pretrained_bert
  model_class: PretrainedBert
  dataio_package: rosetta.datasets.text.masked_text
  dataio_class: MaskedTextDataIO

  train_data_path: ./tests/test_data/sonnets.txt
  eval_data_path: ./tests/test_data/sonnets.txt

  bert_config_file: ./examples/bert/config.json
  tokenizer_name: bert-base-cased
  
  hidden_size: 768
  vocab_size: 30522
  
  num_epochs: 100

dialog_gpt2: &dialog_gpt2
  model_package: examples.dialog_gpt.dialog_gpt2
  model_class: DialogGPT2
  dataio_package: examples.dialog_gpt.dataio
  dataio_class: DialogDataIO

  train_data_path: /workspace/cpfs-data/datasets/open_dialog_data/lccc/lccc-base/cleanwb.json
  eval_data_path: /workspace/cpfs-data/datasets/open_dialog_data/lccc/lccc-base/cleanwb.json

  gpt2_model_conf:
    afn: "gelu"
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    resid_pdrop: 0.1
    initializer_range: 0.02
    layer_norm_epsilon: 1e-05
    n_ctx: 512
    n_embd: 768
    n_head: 12
    n_layer: 12
    n_positions: 513
    n_special: 0
    vocab_size": 21128

  tokenizer_name_or_path: /workspace/cpfs-data/models/reberta_zh_l12_pytorch/vocab.txt

  vocab_size: 21128
  max_contexts_length: 64
  max_response_length: 64
  max_history: 5

  num_epochs: 100
  batch_size: 86
  learning_rate: 5e-5
  lr_warmup_steps: 5000
  lr_constant_steps: 100000
  lr_decay_method: exponential
  lr_decay_steps: 100000
  lr_decay_rate: 0.9

gpt2_chitchat: &gpt2_chitchat
  <<: *dialog_gpt2
  gpt2_model_conf:
    # attn_pdrop: 0.1
    # embd_pdrop: 0.1
    # finetuning_task: none
    # initializer_range: 0.02
    # layer_norm_epsilon: 1e-05
    # n_ctx: 300
    # n_embd: 768
    # n_head: 12
    # n_layer: 10
    # n_positions: 300
    # num_labels: 1
    # output_attentions: false
    # output_hidden_states: false,
    # output_past: true
    # pruned_heads: {}
    # resid_pdrop: 0.1
    # summary_activation: none
    # summary_first_dropout: 0.1
    # summary_proj_to_labels: true
    # summary_type: "cls_index"
    # summary_use_proj: true
    # torchscript: false
    # use_bfloat16: false
    # vocab_size: 13317
    initializer_range: 0.02
    layer_norm_epsilon: 1e-05
    n_ctx: 300
    n_embd: 768
    n_head: 12
    n_layer: 10
    n_positions: 300
    vocab_size: 13317

  pretrained_model: /workspace/project-nas-10251-sh/models/gpt2_chitchat/
  tokenizer_name_or_path: /workspace/project-nas-10251-sh/models/gpt2_chitchat/vocab.txt
  vocab_size: 13317

  repetition_penalty: 1.5
  decode_topk: 8
  decode_topp: 0

gpt2_lccc_base: &gpt2_lccc_base
  <<: *dialog_gpt2
  gpt2_model_conf:
    afn: gelu
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    finetuning_task: none
    id2label:
      0: LABEL_0
      1: LABEL_1
    initializer_range: 0.02
    is_decoder: false
    label2id:
      LABEL_0: 0
      LABEL_1: 1
    layer_norm_epsilon: 1e-05
    n_ctx: 512
    n_embd: 768
    n_head: 12
    n_layer: 12
    n_positions: 513
    n_special: 0
    num_labels: 2
    output_attentions: false
    output_hidden_states: false
    output_past: true
    pruned_heads: {}
    resid_pdrop: 0.1
    summary_activation: none
    summary_first_dropout: 0.1
    summary_proj_to_labels: true
    summary_type: cls_index",
    summary_use_proj: true
    torchscript: false
    use_bfloat16: false
    vocab_size: 13088

  pretrained_model: /workspace/project-nas-10251-sh/models/gpt_chinese/GPT2_LCCC_base
  tokenizer_name_or_path: /workspace/project-nas-10251-sh/models/gpt_chinese/GPT2_LCCC_base/vocab.txt
  vocab_size: 13088
  
  repetition_penalty: 1.5
  decode_topk: 8
  decode_topp: 0
  

dssm_bert: &dssm_bert
  model_package: examples.polyencoder.encoder
  model_class: DSSMBert
  dataio_package: examples.polyencoder.dataio
  dataio_class: ConversationDataIO

  log_dir_prefix: /workspace/cpfs-data/logs

  train_data_path: /workspace/cpfs-data/datasets/open_dialog_data/ubuntu_data/train.txt
  eval_data_path: /workspace/cpfs-data/datasets/open_dialog_data/ubuntu_data/valid.txt

  max_contexts_length: 128
  max_response_length: 64
  max_history: 4

  tokenizer_name_or_path: /workspace/cpfs-data/models/all_bert_models/uncased_L-12_H-768_A-12/vocab.txt
  bert_pretrained_model_path: /workspace/cpfs-data/models/all_bert_models/uncased_L-12_H-768_A-12/
  # bert_config_file: /workspace/cpfs-data/models/all_bert_models/uncased_L-12_H-768_A-12/config.json
  bert_hidden_size: 768

  fc_hidden_size: 64

  num_epochs: 100
  batch_size: 128
  learning_rate: 5e-5

  checkpoint_selector:
    eval_set: eval
    eval_metric: loss
    higher_better: false

speech2text: &speech2text
  model_package: examples.speech.speech2text
  model_class: Speech2TextModel

  dataio_package: examples.speech.dataio
  dataio_class: SpeechDataIO

  train_data_path: /workspace/cpfs-data/datasets/open_speech_data/aishell/data_aishell/manifest.train
  eval_data_path: /workspace/cpfs-data/datasets/open_speech_data/aishell/data_aishell/manifest.dev
  # tokenizer_name_or_path: bert-base-chinese
  tokenizer_name_or_path: /workspace/cpfs-data/models/reberta_zh_l12_pytorch/vocab.txt

  log_dir_prefix: /workspace/cpfs-data/logs

  audio_conf:
    sample_rate: 16000
    feat_type: 'fbank'
    num_mel_bins:  40
    frame_length: 25                      # ms
    frame_shift: 10                       # ms
    dither: 0                             # random dither audio, 0: no dither
    apply_cmvn: True
    delta_order: 2                        # 0: do nothing, 1: add delta, 2: add delta and accelerate
    delta_window_size: 5
  
  encoder_conf:
    # prenet: 'vgg'                         # 'vgg'/'cnn'/''
    # vgg: True                             # 4x reduction on time feature extraction
    d_model: 512
    nhead: 8
    num_layers: 3
    dim_feedforward: 2048
    dropout: 0.1
    activation: "relu"

  decoder_conf:
    d_model: 512
    nhead: 8
    num_layers: 3
    dim_feedforward: 2048
    dropout: 0.1
    activation: "relu"

  attention_conf:
    mode: 'loc'                           # 'dot'/'loc'
    dim: 300
    num_head: 1
    v_proj: False                         # if False and num_head>1, encoder state will be duplicated for each head
    temperature: 0.5                      # scaling factor for attention
    loc_kernel_size: 100                  # just for mode=='loc'
    loc_kernel_num: 10                    # just for mode=='loc'
  
  prenet_name: cnn

  max_seq_len: 64
  feature_dim: 120
  vocab_size: 21128

  ctc_weight: 0.3
  batch_size: 32

  learning_rate: 5e-5

