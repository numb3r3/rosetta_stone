"""Many of the modeling parts here come from the great transformers repository:
https://github.com/huggingface/transformers.
Thanks for the great work! """
from typing import Dict

import torch
from torch import nn
from transformers.modeling_bert import BertConfig, BertModel as _BertModel

from .. import helper
from ..base.model import PretrainedModel


class Bert(_BertModel, PretrainedModel):
    """
    A BERT model that wraps HuggingFace's implementation
    (https://github.com/huggingface/transformers) to fit the LanguageModel class.
    Paper: https://arxiv.org/abs/1810.04805
    """

    def __init__(self, hparams: Dict = None, logger=None):
        if logger is None:
            logger = helper.get_logger(__name__)
        self.logger = logger

        self.bert_config = BertConfig.from_pretrained(hparams["bert_config_file"])

        super().__init__(self.bert_config)

        pretrained_checkpoint_file = hparams.get("pretrained_bert_model", None)
        if pretrained_checkpoint_file:
            self.init_from_checkpoint(pretrained_checkpoint_file)

    def init_from_checkpoint(self, checkpoint_file: str):
        """The following codes are borrowed from
        """
        try:
            state_dict = torch.load(checkpoint_file, map_location="cpu")
        except Exception:
            raise OSError("Unable to load weights from pytorch checkpoint file. ")

        missing_keys = []
        unexpected_keys = []
        error_msgs = []

        # Convert old format to new format if needed from a PyTorch state_dict
        old_keys = []
        new_keys = []
        for key in state_dict.keys():
            new_key = None
            if "gamma" in key:
                new_key = key.replace("gamma", "weight")
            if "beta" in key:
                new_key = key.replace("beta", "bias")
            if new_key:
                old_keys.append(key)
                new_keys.append(new_key)

        for old_key, new_key in zip(old_keys, new_keys):
            state_dict[new_key] = state_dict.pop(old_key)

        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants
        # so we need to apply the function recursively.
        def load(module: nn.Module, prefix=""):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            module._load_from_state_dict(
                state_dict,
                prefix,
                local_metadata,
                True,
                missing_keys,
                unexpected_keys,
                error_msgs,
            )
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + ".")

        # Make sure we are able to load base models as well as derived models (with heads)
        start_prefix = ""
        model_to_load = self
        has_prefix_module = any(
            s.startswith(self.__class__.base_model_prefix) for s in state_dict.keys()
        )
        if not hasattr(self, self.__class__.base_model_prefix) and has_prefix_module:
            start_prefix = self.__class__.base_model_prefix + "."
        if hasattr(self, self.__class__.base_model_prefix) and not has_prefix_module:
            model_to_load = getattr(self, self.__class__.base_model_prefix)

        load(model_to_load, prefix=start_prefix)

        if self.__class__.__name__ != model_to_load.__class__.__name__:
            base_model_state_dict = model_to_load.state_dict().keys()
            head_model_state_dict_without_base_prefix = [
                key.split(self.__class__.base_model_prefix + ".")[-1]
                for key in self.state_dict().keys()
            ]

            missing_keys.extend(
                head_model_state_dict_without_base_prefix - base_model_state_dict
            )

        if len(missing_keys) > 0:
            self.logger.info(
                "Weights of {} not initialized from pretrained model: {}".format(
                    self.__class__.__name__, missing_keys
                )
            )
        if len(unexpected_keys) > 0:
            self.logger.info(
                "Weights from pretrained model not used in {}: {}".format(
                    self.__class__.__name__, unexpected_keys
                )
            )
        if len(error_msgs) > 0:
            raise RuntimeError(
                "Error(s) in loading state_dict for {}:\n\t{}".format(
                    self.__class__.__name__, "\n\t".join(error_msgs)
                )
            )

        self.tie_weights()  # make sure token embedding weights are still tied if needed
